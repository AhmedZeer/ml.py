{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "14HsYneaWKuF07GJP0Bsi5Wmr3Q_4Rz8u",
      "authorship_tag": "ABX9TyO6N5c+2wKvlCJe4Qe4p/d4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedZeer/ml.py/blob/master/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata\n",
        "!pip install portalocker"
      ],
      "metadata": {
        "id": "ZKmRpqKMb1BU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "620f5c97-5ece-4ef4-f0c1-91e20c43daaa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.31.0)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import IMDB\n",
        "train_data = IMDB( split = \"train\" )\n",
        "test_data = IMDB( split = \"test\" )"
      ],
      "metadata": {
        "id": "H2yp49_5beRu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "c1GRTYc_MEaj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import random_split"
      ],
      "metadata": {
        "id": "3RrUap2-cmdr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "train_set, valid_set = random_split( list(train_data), [20000,5000])"
      ],
      "metadata": {
        "id": "oyGGXEYMc5Fx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_ = \"<image> MY PERSonAL oBino ! :D\""
      ],
      "metadata": {
        "id": "zbUKmunN5fm9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "7SU-DiDy7krs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re.sub(\"<[^>]*>\", '', text_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "lHYn9ds17mWG",
        "outputId": "ca772aca-ff40-472b-b4ed-4383ecc22c73"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' MY PERSonAL oBino ! :D'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer( text_ ):\n",
        "  emotes = re.findall(\"(?::|=|;)(?:-)?(?:\\(|\\)|p|d)\",text_.lower())\n",
        "  text_ = re.sub(\"(?::|=|;)(?:-)?(?:\\(|\\)|p|d)\",\"\",text_.lower())\n",
        "  text = re.sub('[\\W+]',' ', text_.lower()) + ' '.join(emotes).replace('-', '')\n",
        "  return text.split()"
      ],
      "metadata": {
        "id": "9M2w0rvV8TgL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(text_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ayJYBojDrFX",
        "outputId": "aa11025d-715f-411d-caf2-a2b62a1e1acc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['image', 'my', 'personal', 'obino', ':d']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.sub('[\\W+]',' ', text_.lower())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "CvEZb8fHCNIN",
        "outputId": "e8570099-4664-43e4-8eb3-c6ecdf86c5a8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' image  my personal obino    d'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, OrderedDict"
      ],
      "metadata": {
        "id": "KcHEEqTq7UqG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_count = Counter()\n",
        "for label, line in train_set:\n",
        "  tokenized = tokenizer(line)\n",
        "  token_count.update(tokenized)"
      ],
      "metadata": {
        "id": "Myzlug4-GCJO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_count.__len__()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZoB2Hr0GgCM",
        "outputId": "279782f8-b7a9-45ea-e036-303221e952e6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68451"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import vocab"
      ],
      "metadata": {
        "id": "4Vr_th8nIQwz"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_tuple_freq = sorted( token_count.items(), key = lambda x:x[1], reverse = True  )"
      ],
      "metadata": {
        "id": "_Dy3f3KzIrKL"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ordered_dict = OrderedDict(sorted_tuple_freq)"
      ],
      "metadata": {
        "id": "6fqrdFV1PlJM"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = vocab(ordered_dict)"
      ],
      "metadata": {
        "id": "y3_pp3AjP5Mh"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.insert_token(\"<unk>\",0)\n",
        "vocab.insert_token(\"<pad>\",1)"
      ],
      "metadata": {
        "id": "8E0r6RWVQFyw"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.set_default_index(0)"
      ],
      "metadata": {
        "id": "oVEj2cN4QNfb"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab['osaudfh']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "He6hExtVQRfF",
        "outputId": "cdbf4d10-744e-46d7-b969-30fd793d67de"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_pipeline = lambda x: [ vocab[token] for token in tokenizer(x) ]\n",
        "label_pipeline = lambda x: True if x == 'pos' else False"
      ],
      "metadata": {
        "id": "ZwllZ7s0RbDU"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "\n",
        "  label_list, text_list, lengths = [],[],[]\n",
        "\n",
        "  for label_, text_ in batch:\n",
        "    label_list.append(label_pipeline(label_))\n",
        "    text_ = torch.tensor(text_pipeline(text_), dtype = torch.int64)\n",
        "    text_list.append(text_)\n",
        "    lengths.append(text_.size(0))\n",
        "\n",
        "  label_list = torch.tensor(label_list).float()\n",
        "  lengths = torch.tensor(lengths)\n",
        "  text_list = nn.utils.rnn.pad_sequence(text_list,batch_first = True)\n",
        "\n",
        "  return text_list, label_list, lengths"
      ],
      "metadata": {
        "id": "1tdaBq-WTtgc"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "5oAS01iia9fe"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_dl = DataLoader( train_set, batch_size = batch_size, shuffle = False, collate_fn = collate_batch )\n",
        "valid_dl = DataLoader( valid_set, batch_size = batch_size, shuffle = False, collate_fn = collate_batch )\n",
        "test_dl = DataLoader( test_data, batch_size = batch_size, shuffle = False, collate_fn = collate_batch )"
      ],
      "metadata": {
        "id": "FjFN_JGFarvN"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "8_kZGLCyMTrd"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_dl))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObOn7wKFfXEI",
        "outputId": "fe39556f-ac4b-4c96-f752-4f2926786f27"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[   2,  299,  446,  ...,   30,  547, 2865],\n",
              "         [  12,    7,    2,  ...,    0,    0,    0],\n",
              "         [  12,   19,   15,  ...,    0,    0,    0],\n",
              "         ...,\n",
              "         [  11,  116,  649,  ...,    0,    0,    0],\n",
              "         [   6,  273,    9,  ...,    0,    0,    0],\n",
              "         [  12,   19,   15,  ...,    0,    0,    0]]),\n",
              " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " tensor([625, 116, 137, 108, 160, 140,  68,  51, 155,  88, 104, 130, 134, 459,\n",
              "         579, 126, 154, 309, 331,  58, 220, 199, 160, 489, 145, 184, 188, 175,\n",
              "         172, 155, 147, 125]))"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_list, label_list, lengths = next(iter(train_dl))\n",
        "print(text_list.shape)\n",
        "print(label_list.shape)\n",
        "print(lengths.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huU4PjhAdzJb",
        "outputId": "3d211584-b00d-46ed-da70-0e171142be91"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 625])\n",
            "torch.Size([32])\n",
            "torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = nn.Embedding( num_embeddings = 10,\n",
        "                         embedding_dim = 3,\n",
        "                         padding_idx = 0 )"
      ],
      "metadata": {
        "id": "AhQejGv3eReU"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_layer, hidden_layer):\n",
        "    super().__init__()\n",
        "    self.rnn = nn.RNN( input_layer, hidden_layer, num_layers = 2, batch_first = True )\n",
        "    self.fc = nn.Linear(hidden_layer, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    _,hidden = self.rnn(x)\n",
        "    out = hidden[-1,:,:]\n",
        "    return self.fc(out)"
      ],
      "metadata": {
        "id": "JHC8EYJfFi6O"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = RNN(64,32)\n",
        "print(rnn)\n",
        "tensor = torch.randn(5,3,64)\n",
        "rnn(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKtDCE6wI3Gv",
        "outputId": "108a128a-dd97-4785-f52d-53ac6931aca2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN(\n",
            "  (rnn): RNN(64, 32, num_layers=2, batch_first=True)\n",
            "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.5131],\n",
              "        [ 0.2918],\n",
              "        [ 0.3124],\n",
              "        [-0.4557],\n",
              "        [-0.2058]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_size, rnn_hidden, fc_hidden):\n",
        "    super().__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_size, padding_idx = 0 )\n",
        "    self.rnn = nn.LSTM( embed_size, rnn_hidden, batch_first = True )\n",
        "    self.fc1 = nn.Linear( rnn_hidden, fc_hidden )\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(  fc_hidden, 1 )\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward( self, text, lengths ):\n",
        "    out = self.embed(text)\n",
        "    out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), batch_first = True, enforce_sorted = False )\n",
        "    out, (hidden,cell) = self.rnn(out)\n",
        "    out = hidden[-1,:,:]\n",
        "    out = self.fc1(out)\n",
        "    out = self.fc2(out)\n",
        "    out = self.relu(out)\n",
        "    return self.sigmoid(out)"
      ],
      "metadata": {
        "id": "25S6Tqkb3TTB"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuhvR1XcWK0F",
        "outputId": "91ee24a7-640d-4b14-e189-5e854968fc57"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68453"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 32\n",
        "fc_hidden = 64\n",
        "rnn_hidden = 64\n",
        "torch.manual_seed(0)\n",
        "rnn = RNN(vocab_size, embed_size=embed_size, rnn_hidden = rnn_hidden, fc_hidden = fc_hidden )"
      ],
      "metadata": {
        "id": "owziqE5aWQkP"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tIULpzmXSFL",
        "outputId": "e39367d3-6e50-4882-b5f0-a9898b0d3c0d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embed): Embedding(68453, 32, padding_idx=0)\n",
              "  (rnn): LSTM(32, 64, batch_first=True)\n",
              "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optim = torch.optim.Adam(rnn.parameters(), lr = 0.001)\n",
        "loss_fn = nn.BCELoss()\n",
        "def train(dataloader):\n",
        "  total_acc = 0\n",
        "  total_loss = 0\n",
        "  rnn.train()\n",
        "  for text_batch, label_batch, lengths in dataloader:\n",
        "    rnn.zero_grad()\n",
        "    pred = rnn(text_batch, lengths).squeeze()\n",
        "    # print(pred)\n",
        "    # print(label_batch)\n",
        "    cost = loss_fn(pred,label_batch)\n",
        "    cost.backward()\n",
        "    optim.step()\n",
        "    total_acc  += (( pred >= 0.5 ).float() == label_batch).float().sum()\n",
        "    total_loss += cost.item() * label_batch.size(0)\n",
        "  return( total_acc / len(dataloader.dataset), total_loss / len(dataloader.dataset) )"
      ],
      "metadata": {
        "id": "9q0rZbCjYbLz"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(dataloader):\n",
        "  with torch.no_grad:\n",
        "    for text_batch, label_batch, lengths in dataloader:\n",
        "      pred = rnn(text_batch, lengths).squeeze()\n",
        "      cost = loss_fn(pred,label_batch)\n",
        "      total_acc  += (( pred >= 0.5 ).float() == label_batch).float().sum()\n",
        "      total_loss += cost.item() * label_batch.size(0)\n",
        "\n",
        "  return( total_acc / len(dataloader.dataset), total_loss / len(dataloader.dataset) )"
      ],
      "metadata": {
        "id": "bFmBQUhFbg_2"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://www.gutenberg.org/files/1268/1268-0.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33Mj6esbsPrp",
        "outputId": "900109ed-578d-418a-9876-bc871cde27be"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 1143k  100 1143k    0     0  4547k      0 --:--:-- --:--:-- --:--:-- 4557k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "IQjYCuPGvesS"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/1268-0.txt\", \"r\") as fp:\n",
        "  text = fp.read()\n",
        "\n",
        "start_indx = text.find('THE MYSTERIOUS ISLAND')\n",
        "end_indx = text.find('End of the Project Gutenberg')\n",
        "\n",
        "text = text[start_indx: end_indx]"
      ],
      "metadata": {
        "id": "Po5GypK-vh24"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_text = set(text)"
      ],
      "metadata": {
        "id": "luk4ShhgwBym"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique Chars:\",len(char_text))\n",
        "print(\"Chars:\",len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qZT6jIhw-av",
        "outputId": "ff71d086-fc99-47cf-902b-ce2ede3b98ee"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Chars: 85\n",
            "Chars: 1130711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_sorted = sorted(char_text)"
      ],
      "metadata": {
        "id": "98535gUvwSj3"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char2int = {ch:i for i,ch in enumerate(char_sorted)}"
      ],
      "metadata": {
        "id": "3US7swg2xT3i"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_array = np.array(char_sorted)"
      ],
      "metadata": {
        "id": "mJoR_wL9z7nn"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text = np.array([ char2int[ch] for ch in text ])"
      ],
      "metadata": {
        "id": "8LcRFg2dza-d"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ex in encoded_text[:10]:\n",
        "  print(\"{} -> {}\".format(ex,char_array[ex]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEFnbgunz1vD",
        "outputId": "91bf08a1-2bf9-4000-8b08-a76a04f786a2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48 -> T\n",
            "36 -> H\n",
            "33 -> E\n",
            "1 ->  \n",
            "41 -> M\n",
            "53 -> Y\n",
            "47 -> S\n",
            "48 -> T\n",
            "33 -> E\n",
            "46 -> R\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "NGYQra7E3g_N"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 40\n",
        "chunk_size = seq_len + 1\n",
        "text_chunks = [encoded_text[i:chunk_size+i] for i in range( len(encoded_text) - chunk_size )]"
      ],
      "metadata": {
        "id": "CxQaw15I3qCh"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "  def __init__(self, text_chunks):\n",
        "    self.text_chunks = text_chunks\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text_chunks)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    text_chunk = self.text_chunks[idx]\n",
        "    return ( text_chunk[:-1].long(), text_chunk[1:].long() )\n",
        "\n",
        "seq_dataset = TextDataset(torch.tensor(text_chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89c3hnk25og4",
        "outputId": "ade85340-c298-468b-e9ef-3b20005da0c4"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-66-0f0488180fc5>:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
            "  seq_dataset = TextDataset(torch.tensor(text_chunks))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "eeYfnOAO_Yv6"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DataLoader( seq_dataset, batch_size = 32, shuffle = True, drop_last = True )"
      ],
      "metadata": {
        "id": "0pIfn6hQ_oXW"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "jVS6ET06ABCa"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN( nn.Module ):\n",
        "\n",
        "  def __init__(self, vocab_size, embed_dim, rnn_hidden_num ):\n",
        "    super().__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "    self.rnn_hidden_size = rnn_hidden_num\n",
        "    self.rnn = nn.LSTM( embed_size, self.rnn_hidden_size, batch_first = True )\n",
        "    self.fc = nn.Linear(self.rnn_hidden_size, vocab_size)\n",
        "\n",
        "  def forward(self, x, hidden, cell):\n",
        "    out = self.embed(x).unsqueeze(1)\n",
        "    out, (hidden,cell) = self.rnn(out,(hidden,cell))\n",
        "    out = self.fc(out).reshape(out.size(0),-1)\n",
        "    return out, hidden, cell\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    hidden = torch.zeros( 1, batch_size, self.rnn_hidden_size )\n",
        "    cell   = torch.zeros( 1, batch_size, self.rnn_hidden_size )\n",
        "    return (hidden, cell)"
      ],
      "metadata": {
        "id": "ORUcSiIyEdCs"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(char_array)\n",
        "embed_size = 256\n",
        "rnn_hidden_num = 512\n",
        "model = RNN(vocab_size, embed_size, rnn_hidden_num)"
      ],
      "metadata": {
        "id": "uLDuUwvWOe-I"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIhB0iXaPa1s",
        "outputId": "46216ca3-44e3-4fbc-91ea-453eb75609bf"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embed): Embedding(85, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=85, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9U7NfydjaS8e",
        "outputId": "d5ba5231-870b-4725-97df-f04c385b5f68"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embed): Embedding(85, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=85, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optim = torch.optim.Adam(model.parameters(),0.001)"
      ],
      "metadata": {
        "id": "yHibdUnLPb2H"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 10000\n",
        "batch_size = 32\n",
        "for e in range(n_epochs):\n",
        "  hidden,cell = model.init_hidden(batch_size)\n",
        "  hidden,cell = hidden.to(device), cell.to(device)\n",
        "  seq_batch, target_batch = next(iter(train_dl))\n",
        "  seq_batch, target_batch = seq_batch.to(device), target_batch.to(device)\n",
        "  loss = 0\n",
        "  optim.zero_grad()\n",
        "  for c in range(seq_len):\n",
        "    pred, hidden, cell = model(seq_batch[:,c], hidden, cell)\n",
        "    loss += loss_fn(pred,target_batch[:,c])\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "  loss = loss.item() / seq_len\n",
        "  if( e % 100 == 0 ):\n",
        "    print(loss)"
      ],
      "metadata": {
        "id": "b-KitiA6ruQB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "db355dac-2a19-4482-9a44-551b46a3148a"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.422719955444336\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-1de64897fd1c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions.categorical import Categorical"
      ],
      "metadata": {
        "id": "NaQBA__vtpOu"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, starting_str, len_generated = 200, scale_factor = 1 ):\n",
        "\n",
        "  encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
        "  encoded_input = torch.reshape(encoded_input, (1,-1))\n",
        "  generated_str = starting_str\n",
        "\n",
        "  model.eval()\n",
        "  hidden,cell = model.init_hidden(1)\n",
        "  for c in range(len(starting_str)-1):\n",
        "    pred, hidden, cell = model(encoded_input[:,c].view(1),hidden,cell)\n",
        "\n",
        "  last_char = encoded_input[:,-1]\n",
        "  for i in range(len_generated):\n",
        "    logits,hidden,cell = model( last_char.view(1), hidden, cell )\n",
        "    logits = torch.squeeze(logits, 0)\n",
        "    scaled_logits = logits * scale_factor\n",
        "    m = Categorical(logits = logits)\n",
        "    last_char = m.sample()\n",
        "    generated_str += str(char_array[last_char])\n",
        "\n",
        "  return generated_str\n",
        "# print(sample(model,\"Hello\"))"
      ],
      "metadata": {
        "id": "u8WWqBILMM_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ce895c3-6690-4eb7-93e4-3d0e618e8eb5"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hellow might be so,” replied the engineer, might have taken a moments he has all his putrated where their fury, eyes improve from\n",
            "the clear on a spars right as far in a mercy, striking the necessary\n",
            "peccar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKUd6Vwfwrre",
        "outputId": "0b30883e-5614-44f1-d745-47f4cc403b6e"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embed): Embedding(85, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=85, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"lstm.pt\", \"wb\") as f:\n",
        "  torch.save(model,f)"
      ],
      "metadata": {
        "id": "4tzc8ZZfwv_t"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load torch model\n",
        "\n",
        "with open(\"/content/drive/MyDrive/lstm.pt\", \"rb\") as f:\n",
        "  model = torch.load(f,map_location=torch.device('cpu'))\n"
      ],
      "metadata": {
        "id": "tvSzoiG7xCmN"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: change device of model\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX8fKGgFxIvi",
        "outputId": "45f480dc-f318-4a2b-9162-95db963243ed"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embed): Embedding(85, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=85, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(model,\"Hello\", scale_factor = 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cxqh-c1skm_M",
        "outputId": "e3aef0ab-44ce-410d-f4e5-a28bc33a13c4"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hellow, when it was to be feet, was thus\n",
            "living two granite! Spilett\n",
            "remarked that the cotcembers tust, but he observed frigates.\n",
            "\n",
            "On the 7th of October, the rapid part had not served to\n",
            "have an abundance \n"
          ]
        }
      ]
    }
  ]
}